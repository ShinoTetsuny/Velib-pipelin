# üöÄ Guide Installation Cluster Velib - Pour les Coll√®gues

## üéØ **Pr√©requis**
- Docker Desktop install√© et d√©marr√©
- PowerShell (ou Terminal Windows)
- Au minimum 6GB RAM libre

## ‚ö° **Installation en 3 √©tapes**

### 1. **D√©marrer le cluster**
```powershell
.\deploy-cluster.ps1
```
‚Üí Lance tous les conteneurs (namenode, spark, mongodb...)

### 2. **Initialiser les donn√©es**
```powershell
.\init-hdfs.ps1
```
‚Üí Copie velib.csv vers HDFS et cr√©√© l'arborescence

### 3. **D√©velopper vos parties**
Maintenant vous pouvez travailler sur votre partie :
- **Batch** : Cr√©er des scripts d'analyse Spark
- **Streaming** : D√©velopper le traitement temps r√©el

## üåê **Interfaces disponibles**

| Interface | URL | Description |
|-----------|-----|-------------|
| Hadoop | http://localhost:9870 | Syst√®me de fichiers HDFS |
| Spark | http://localhost:8080 | Cluster de traitement |
| YARN | http://localhost:8088 | Gestionnaire de jobs |

## üîß **Commandes utiles**

### Initialisation
```powershell
.\init-hdfs.ps1          # Initialiser HDFS avec les donn√©es
```

### D√©veloppement des jobs (√† faire)
```powershell
# √Ä cr√©er par l'√©quipe BATCH
.\run-batch-analysis.ps1

# √Ä cr√©er par l'√©quipe STREAMING  
.\run-streaming.ps1
```

### D√©pannage
```powershell
# Tester le cluster
.\test-cluster.ps1

# Voir les logs d'un service
docker-compose logs spark-master

# Red√©marrer le cluster
docker-compose restart

# Arr√™ter le cluster
docker-compose down
```

### Acc√©der aux conteneurs
```powershell
# HDFS (pour v√©rifier les fichiers)
docker exec -it namenode bash
hdfs dfs -ls /users/ipssi/input/velib2/

# MongoDB (pour voir les r√©sultats)
docker exec -it mongodb-ipssi mongosh
use velib
db.batch_results.find().limit(5)

# Spark Master
docker exec -it spark-master bash
```

## üìä **D√©veloppement**

### Pour le **BATCH** :
- Modifier : `app/batch-spark.py`
- Tester : `.\run-jobs.ps1 batch`
- R√©sultats : MongoDB collection `velib.batch_results`

### Pour le **STREAMING** :
- Modifier : `streaming-velib.py` (dans le dossier racine)  
- Tester : `.\run-jobs.ps1 streaming`
- R√©sultats : Console + MongoDB

### Pour l'**INFRASTRUCTURE** :
- Config cluster : `docker-compose.yml`
- Variables Hadoop : `hadoop.env`
- Scripts : `deploy-cluster.ps1`, `run-jobs.ps1`

## ‚ùå **R√©solution des probl√®mes courants**

### Le cluster ne d√©marre pas
```powershell
# V√©rifier Docker
docker --version

# Nettoyer et relancer
docker-compose down -v
.\deploy-cluster.ps1
```

### "hdfs command not found"
‚Üí **Normal**, c'est pour √ßa qu'on utilise `docker exec namenode hdfs ...`

### Job Spark √©choue
```powershell
# V√©rifier que l'init est fait
docker exec namenode hdfs dfs -ls /users/ipssi/input/velib2/

# Voir les logs d√©taill√©s
docker-compose logs spark-master
```

### Ports occup√©s
```powershell
netstat -an | findstr "8080\|8088\|9870"
# Si occup√©s, arr√™ter les autres services
```

## üéâ **C'est pr√™t !**

Le cluster est maintenant op√©rationnel. Chacun peut travailler sur sa partie :
- **Batch** : Analyses complexes des donn√©es historiques
- **Streaming** : Traitement temps r√©el des nouvelles donn√©es  
- **Infrastructure** : Monitoring, optimisation, s√©curit√©

**Bon d√©veloppement ! üöÄ**
